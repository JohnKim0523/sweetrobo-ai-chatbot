import os
import streamlit as st
from dotenv import dotenv_values
import json
import uuid
from datetime import datetime
from openai import OpenAI
from pinecone import Pinecone
import numpy as np
import re
from difflib import SequenceMatcher
import math

# === Load keys from secrets or .env ===
config = dotenv_values(".env")
openai_key = st.secrets.get("OPENAI_API_KEY", config.get("OPENAI_API_KEY"))
pinecone_key = st.secrets.get("PINECONE_API_KEY", config.get("PINECONE_API_KEY"))

if not openai_key or not pinecone_key:
    raise ValueError("âŒ Missing OPENAI_API_KEY or PINECONE_API_KEY")

# === Config ===
embedding_model = "text-embedding-3-small"
CONFIDENCE_THRESHOLD = 0.6
ESCALATION_RESPONSE = "I wasnâ€™t able to find any additional information. Iâ€™m escalating this to our support team so they can follow up with you directly."

# === Init clients ===
client = OpenAI(api_key=openai_key)
pc = Pinecone(api_key=pinecone_key)
index = pc.Index("sweetrobo-ai")

# === Global Chat State ===
th_state = {
    "thread_id": None,
    "machine_type": None,
    "used_matches_by_thread": {},
    "conversation_history": [],
    "solution_attempts": {},
    "email_collected": False,
}

def is_valid_email(email):
    return re.match(r"[^@]+@[^@]+\.[^@]+", email)

def track_solution_failure():
    thread_id = th_state["thread_id"]
    th_state["solution_attempts"][thread_id] = (
        th_state["solution_attempts"].get(thread_id, 0) + 1
    )
    return th_state["solution_attempts"][thread_id]

def handle_email_logic(user_question):
    if "@" in user_question:
        if is_valid_email(user_question.strip()):
            th_state["email_collected"] = True
            return "âœ… Email received. Adding the employee to the chat session."
        else:
            return "âš ï¸ That doesnâ€™t look like a valid email. Please check it and try again."

    if any(phrase in user_question.lower() for phrase in ["this didnt solve", "didn't work", "not fixed", "still broken"]):
        if not th_state["email_collected"]:
            return "Please provide the employee's email address so we can escalate this issue."

    return None

def initialize_chat(selected_machine: str):
    th_state["thread_id"] = str(uuid.uuid4())
    machine_mapping = {
        "cotton candy": "COTTON_CANDY",
        "ice cream": "ROBO_ICE_CREAM",
        "balloon bot": "BALLOON_BOT",
        "candy monster": "CANDY_MONSTER",
        "popcart": "POPCART",
        "mr. pop": "MR_POP",
        "marshmallow spaceship": "MARSHMALLOW_SPACESHIP",
    }
    machine_type = machine_mapping.get(selected_machine.strip().lower())
    if not machine_type:
        raise ValueError("Unknown machine type selected.")
    th_state["machine_type"] = machine_type
    return {"thread_id": th_state["thread_id"], "machine_type": machine_type}

def is_question_too_vague(user_q):
    specific_keywords = {
        'machine', 'candy', 'cotton', 'issue', 'sugar', 'after', 'settings', 'check',
        'error', 'please', 'stick', 'ensure', 'screen', 'support', 'video', 'power',
        'system', 'clean', 'sensor', 'provide', 'test', 'burner', 'further', 'team',
        'design', 'during', 'could', 'properly', 'furnace', 'issues', 'motor',
        'number', 'persists', 'time', 'correct', 'cleaning', 'nozzle', 'portal',
        'admin', 'update', 'sure', 'working', 'water', 'sticks', 'nayax', 'send',
        'correctly', 'showing', 'help', 'confirm', 'payment', 'heating', 'machines',
        'replacement', 'device', 'balloon', 'restart', 'problem', 'change', 'stuck',
        'through', 'using', 'menu', 'verify', 'connected', 'alert', 'inventory',
        'temperature', 'address', 'prevent', 'remove', 'resolve', 'software',
        'contact', 'robo', 'before', 'again', 'cable', 'data', 'access', 'down',
        'reset', 'card', 'setting', 'alerts', 'sync', 'process', 'call', 'print',
        'sweet', 'clear', 'causing', 'right', 'replace', 'internal', 'loose',
        'assistance', 'production', 'getting', 'inside'
    }
    if any(word in user_q.lower() for word in specific_keywords):
        return False

    prompt = f"""You are a support assistant. You will be given a user's message.

You must check if the message matches any known vague expressions from the list below. These expressions include messages that are too general, do not mention any part, symptom, or error code, and cannot be acted on without clarification.

- my machine is not working
- nothing is happening
- i need help
- itâ€™s broken
- iâ€™m having an issue
- somethingâ€™s wrong
- please assist
- i canâ€™t use it
- wonâ€™t start
- stopped working
- not working again
- whatâ€™s going on?
- can you fix this?
- having trouble
- doesnâ€™t respond
- need support
- weird behavior
- wonâ€™t power on
- power issue
- broken again
- doesnâ€™t run
- not behaving right
- crashes
- keeps glitching
- doesnâ€™t load
- not responding
- app not working
- still not fixed
- confused
- wonâ€™t go
- machine failure
- nothing loads
- restart didnâ€™t work
- not functional
- why isnâ€™t it working
- how come itâ€™s broken
- just doesnâ€™t work
- problem with it
- general malfunction
- help me
- itâ€™s acting weird
- glitching again
- it wonâ€™t do anything
- i give up
- issue persists
- same problem again
- problem still exists
- canâ€™t figure it out
- doesnâ€™t work right
- need some help

User message: \"{user_q}\"

Respond only with:
- "yes"
- "no"
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4", messages=[{"role": "user", "content": prompt}], temperature=0
        )
        return "yes" in response.choices[0].message.content.lower()
    except Exception:
        return False

def is_similar_answer(ans1, ans2, threshold=0.85):
    return SequenceMatcher(None, ans1, ans2).ratio() >= threshold

def bulletify_if_long(answer):
    parts = re.split(r'(?<=\.)\s+', answer.strip())
    if len(parts) < 3:
        return answer
    bullets = [f"â€¢ {p.strip()}" for p in parts if p.strip()]
    return "\n\n".join(bullets)

def is_related(user_q, match_q, match_a):
    full_text = match_q + " " + match_a
    ratio = SequenceMatcher(None, user_q.lower(), full_text.lower()).ratio()
    
    # ğŸ” Debug output
    print(f"ğŸ”— [is_related] Similarity between user_q and match Q+A:")
    print(f"   - User Q:    {user_q}")
    print(f"   - Match Q:   {match_q}")
    print(f"   - Match A:   {match_a[:100]}...")  # Trim long answers
    print(f"   - Similarity Ratio: {ratio:.3f}")
    print(f"   - Pass? {'âœ…' if ratio > 0.3 else 'âŒ'}\n")
    
    return ratio > 0.05  # You can tune this threshold (e.g., 0.25 or 0.35)

def is_already_given(answer, history, threshold=0.85):
    for entry in history:
        if entry["role"] == "assistant":
            prev_ans = entry["content"]
            if SequenceMatcher(None, answer.strip(), prev_ans.strip()).ratio() >= threshold:
                return True
    return False

def build_followup_query(user_q: str, original_q: str):
    """
    Returns enriched question and whether enrichment was applied.
    """
    cleaned = user_q.strip().lower()
    if len(cleaned.split()) <= 4:
        return f"{user_q.strip()} â€” referring to: {original_q.strip()}", True
    return user_q.strip(), False

def handle_followup_with_existing_matches(user_q, thread_id):
    match_pointer = th_state.get("match_pointer", 1)
    used_info = th_state["used_matches_by_thread"][thread_id]
    all_matches = used_info["all_matches"]
    original_question = used_info["original_question"]
    
    print(f"ğŸ” [DEBUG] Starting follow-up match filtering...")
    print(f"ğŸ”¢ Match pointer: {match_pointer}")
    print(f"ğŸ“Š Total matches available: {len(all_matches)}")
    
        # ğŸ›‘ No matches left to try â€” escalate immediately
    if match_pointer >= len(all_matches):
        print("âš ï¸ All follow-up matches have been exhausted. Escalating.")
        return ESCALATION_RESPONSE

    # Decide whether to enrich the follow-up text with the original question
    query_for_related, used_enriched = build_followup_query(user_q, original_question)

    filtered_qas = []
    seen_ids = set()

    # Pass 1: try with chosen query (raw follow-up OR enriched, depending on heuristic)
    for i in range(match_pointer, len(all_matches)):
        match, usefulness, confidence = all_matches[i]
        answer = match.metadata.get("a", "[No A]").strip()
        if not answer:
            continue
        
        print(f"\nğŸ” [DEBUG] Checking match index {i}")
        print(f"ğŸ†” Match ID: {match.id}")
        print(f"â“ Q: {match.metadata.get('q', '')}")
        print(f"ğŸ’¬ A: {answer[:100]}...")

        q = match.metadata.get("q", "")
        a = match.metadata.get("a", "")

        related = is_related(query_for_related, q, a)
        already_given = is_already_given(a, th_state["conversation_history"])
        
        print(f"ğŸ”— Related? {related}")
        print(f"ğŸ§  Already Given? {already_given}")

        print(f"ğŸ§ª FOLLOW-UP FILTER (pass1 {'enriched' if used_enriched else 'raw'}):")
        print(f"- Related: {related}")
        print(f"- Already Given: {already_given}")
        print(f"- Q: {q}")
        print(f"- A: {a[:80]}...\n")

        if related and not already_given:
            th_state["match_pointer"] = i + 1  # advance pointer
            filtered_qas.append(a)
            seen_ids.add(match.id)

    # Pass 2 (fallback): if we *did not* enrich on pass1 AND got nothing, try enriched
    if not filtered_qas and not used_enriched:
        enriched_again, _ = build_followup_query("", original_question)  # force enrich
        for i in range(match_pointer, len(all_matches)):
            match, usefulness, confidence = all_matches[i]
            if match.id in seen_ids:
                continue
            a = match.metadata.get("a", "[No A]").strip()
            if not a:
                continue
            q = match.metadata.get("q", "")
            related = is_related(enriched_again, q, a)
            already_given = is_already_given(a, th_state["conversation_history"])

            print(f"ğŸ§ª FOLLOW-UP FILTER (pass2 enriched):")
            print(f"- Related: {related}")
            print(f"- Already Given: {already_given}")
            print(f"- Q: {q}")
            print(f"- A: {a[:80]}...\n")

            if related and not already_given:
                th_state["match_pointer"] = i + 1
                filtered_qas.append(a)
                seen_ids.add(match.id)

    # Nothing found after both passes â†’ clarification or escalation
    if not filtered_qas:
        print("âš ï¸ No valid follow-up matches found. Entering fallback mode.")
        failure_count = track_solution_failure()
        if failure_count == 1:
            return ("Thanks for letting me know. Could you describe exactly what didnâ€™t work "
                    "or whatâ€™s still happening (e.g., error still shows, part stuck, no heat)?")
        if failure_count >= 2:
            return "This seems persistent. Escalating to a human agent now. Please wait..."
        # fallback safety
        return "Sorry, I couldnâ€™t find any new helpful info for this issue. Escalating this to our support team. Please hold on."

    # Summarize / return one or more answers
    if len(filtered_qas) == 1:
        final_answer = filtered_qas[0]
    else:
        combined_input = "\n\n".join(filtered_qas)
        simple_q_phrases = ["can i", "do you", "does it", "is there", "are there", "can we", "is it possible"]
        is_simple_question = any(original_question.lower().startswith(p) for p in simple_q_phrases)
        is_short_answer = len(filtered_qas[0].split()) <= 12 and filtered_qas[0].lower().startswith(("yes", "no", "sorry", "unfortunately"))
        try:
            if is_simple_question and is_short_answer:
                final_answer = filtered_qas[0]
            else:
                gpt_prompt = f"""
You are a helpful AI assistant for customer support. The user said the initial solution didnâ€™t work.

You are given up to 5 technical answers. Your job is to summarize only the most helpful 1â€“3 suggestions.

Instructions:
- Provide only the steps needed to address the issue (1 to 5 max).
- Use bullet points (â€¢), not numbers.
- Be concise and do not repeat instructions.
- Do not say â€œif that doesnâ€™t workâ€ â€” that will be appended later.

User Question:
{original_question}

Answer References:
{combined_input}

Final helpful answer:
"""
                gpt_response = client.chat.completions.create(
                    model="gpt-4",
                    messages=[{"role": "user", "content": gpt_prompt}],
                    temperature=0.3,
                )
                final_answer = gpt_response.choices[0].message.content.strip()
                final_answer = final_answer.replace("â€¢", "\n\nâ€¢")
        except Exception:
            final_answer = "Sorry, no answer was found. Escalating to our support team now."

    final_answer += "\n\nIf this didnâ€™t resolve the issue, let me know."
    th_state["conversation_history"].append({"role": "user", "content": user_q})
    th_state["conversation_history"].append({"role": "assistant", "content": final_answer})
    return final_answer

def get_question_similarity_boost(user_q, candidate_q):
    ratio = SequenceMatcher(None, user_q.lower(), candidate_q.lower()).ratio()
    if ratio >= 0.95:
        return 0.5
    elif ratio >= 0.85:
        return 0.3
    elif ratio >= 0.7:
        return 0.1
    return 0

import math

# Helper: cosine similarity
def cosine_similarity(vec1, vec2):
    dot = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = math.sqrt(sum(a * a for a in vec1))
    norm2 = math.sqrt(sum(b * b for b in vec2))
    return dot / (norm1 * norm2 + 1e-10)

def is_same_topic(user_q, candidate_q):
    prompt = f"""You are helping match a user's support question with known help topics. Consider questions to be "the same" if they are about the same real-world issue, even if the wording is different.

Are these two questions about the same issue?

User: "{user_q}"
Candidate: "{candidate_q}"

Respond only with "yes" or "no".
"""    
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
        )
        result = response.choices[0].message.content.strip().lower()
        print(f"ğŸ§  GPT Topic Check: '{candidate_q}' â†’ {result}")
        return "yes" in result
    except Exception as e:
        print(f"âš ï¸ GPT topic check failed: {e}")
        return False

def fetch_valid_matches(query_embedding, previous_ids, error_code_filter, query_text):
    filter_query = {"machine_type": {"$eq": th_state["machine_type"]}}
    if error_code_filter:
        filter_query["error_codes"] = {"$in": [str(error_code_filter)]}
    else:
        filter_query["error_codes"] = {"$exists": False}

    all_valid = []
    seen_answers = set()
    keyword_focus = "stick" if "stick" in query_text.lower() else None

    results = index.query(
        vector=query_embedding,
        top_k=20,
        include_metadata=True,
        include_values=True,
        filter=filter_query,
    )

    for match in results.matches:
        if match.id in previous_ids:
            continue

        meta = match.metadata
        usefulness_score = meta.get("usefulness_score", 0)
        confidence = meta.get("confidence", 0.0)
        answer = meta.get("a", "")
        tags = meta.get("tags", [])
        candidate_q = meta.get("q", "")

        if usefulness_score >= 7 and confidence >= CONFIDENCE_THRESHOLD:
            if keyword_focus and not any(keyword_focus in t.lower() for t in tags):
                continue
            if isinstance(match.values, list) and len(match.values) == 1536:
                if answer not in seen_answers:
                    # Compute embedding similarity to candidate_q
                    response = client.embeddings.create(model=embedding_model, input=[candidate_q])
                    candidate_q_embedding = response.data[0].embedding
                    similarity_score = cosine_similarity(query_embedding, candidate_q_embedding)

                    print(f"ğŸ¯ Cosine Sim = {similarity_score:.3f} between:\nâ†’ User: {query_text}\nâ†’ Q:    {candidate_q}\n")

                    if similarity_score < 0.50:
                        continue  # âŒ Skip irrelevant Qs

                    boost = get_question_similarity_boost(query_text, candidate_q)
                    boosted_score = usefulness_score + boost
                    all_valid.append((match, boosted_score, confidence))

                    # âœ… Match logging (preserved)
                    print(f"âœ… Matched Q: {candidate_q}\nâ†’ Boosted score: {boosted_score:.2f} (Usefulness: {usefulness_score}, Boost: {boost:.2f}, Confidence: {confidence:.2f})\n")

                    seen_answers.add(answer)

    all_valid.sort(key=lambda x: (-x[1], -x[2]))  # Sort by boosted_score, then confidence

    # ğŸ” Apply GPT topic check to top 10
    filtered_with_gpt = []
    for match, boosted_score, confidence in all_valid[:10]:  # Only check top 10 to keep it fast
        q = match.metadata.get("q", "")
        if is_same_topic(query_text, q):
            filtered_with_gpt.append((match, boosted_score, confidence))
        else:
            print(f"ğŸš« Rejected by GPT topic match: {q}")

    filtered_with_gpt.sort(key=lambda x: -x[0].score)  # Sort by cosine similarity DESC
    return filtered_with_gpt[:5]

def is_followup_message(user_q):
    followup_phrases = {
        "didn't work", "didnt work", "didn't help", "didnt help",
        "still broken", "not fixed", "didnâ€™t resolve", "not working",
        "it did not resolve", "it did not work", "still not fixed",
        "that didnâ€™t help", "that did not work", "this didnâ€™t help",
        "not resolved", "didnâ€™t fix", "didnt fix", "didnâ€™t solve",
        "wasnâ€™t fixed", "this didnâ€™t fix", "that didnâ€™t fix"
    }
    normalized = user_q.lower().replace("â€™", "'").strip()

    if any(p in normalized for p in followup_phrases):
        return True

    # âœ… Fallback: ask GPT if it's a follow-up intent
    prompt = f"""You're a support assistant. Determine if the following user message is a follow-up complaint â€” meaning that a previous solution attempt did not work and the user is still seeking help.

Respond ONLY with "yes" or "no".

User message:
"{user_q}"
"""
    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        return "yes" in response.choices[0].message.content.lower()
    except Exception:
        return False

def run_chatbot_session(user_question: str) -> str:
    email_response = handle_email_logic(user_question)
    if email_response:
        return email_response

    thread_id = th_state["thread_id"]
    used_matches_by_thread = th_state["used_matches_by_thread"]
    
    if not thread_id:
        return "âš ï¸ Please start a new support session by selecting the machine first."

    # ğŸ§  Normalize short numeric input like "4012"
    if re.fullmatch(r"\d{4,}", user_question):
        user_question = f"how to fix error {user_question}"
        
    # ğŸ” Check for follow-up intent
    is_followup = is_followup_message(user_question)
        
    # ğŸš« Check for vague question (clarification fallback)
    if not is_followup and is_question_too_vague(user_question):
        return ("Thatâ€™s a bit too general. Could you describe exactly whatâ€™s going wrong "
                "(e.g., error code, what part is malfunctioning, or whatâ€™s not working as expected)?")

    print(f"ğŸ§µ thread_id: {th_state['thread_id']}")
    print(f"ğŸ“ has_all_matches: {'all_matches' in used_matches_by_thread.get(th_state['thread_id'], {})}")
    print(f"ğŸ” is_followup: {is_followup}")

    # âœ… If it's a follow-up, reuse prior results
    if is_followup:
        if thread_id in used_matches_by_thread and "all_matches" in used_matches_by_thread[thread_id]:
            return handle_followup_with_existing_matches(user_question, thread_id)
        else:
            return "I'm still trying to find the best solution. Could you restate the issue in more detail?"

    # ğŸ§  First-time question: embed and fetch matches
    response = client.embeddings.create(model=embedding_model, input=[user_question])
    query_embedding = response.data[0].embedding
    previous_ids = set()
    original_question = user_question
    match = re.search(r"(\d{4,})", user_question)
    error_code_filter = int(match.group(1)) if "error" in user_question.lower() and match else None

    top_matches = fetch_valid_matches(query_embedding, previous_ids, error_code_filter, original_question)

    # ğŸ’¾ Save matches for future follow-up
    th_state["used_matches_by_thread"][thread_id] = {
        "embedding": query_embedding,
        "used_ids": set(),
        "error_filter": error_code_filter,
        "original_question": original_question,
        "all_matches": top_matches,
    }
    th_state["match_pointer"] = 1  # Start at second match for follow-up

    # âœ… Optional: filter very low-similarity matches
    filtered_top = []
    for match, boosted_score, confidence in top_matches:
        cosine_sim = match.score
        q_text = match.metadata.get("q", "")
        a_text = match.metadata.get("a", "[No A]").strip()

        print(f"ğŸ¯ Cosine Sim = {cosine_sim:.3f} between:")
        print(f"â†’ User: {user_question}")
        print(f"â†’ Q:    {q_text}")
        print(f"â†’ A:    {a_text[:120]}...\n")
        if cosine_sim < 0.4:
            break
        filtered_top.append((match, boosted_score, confidence))
    top_matches = filtered_top

    # âŒ No valid matches
    if not top_matches:
        print("âŒ No top matches found after similarity filtering.")
        return "Sorry, I couldnâ€™t find a helpful answer. Can you rephrase the question with more details?"

    # âœ… Present the best GPT-filtered match (highest cosine)
    if top_matches:
        best_match = top_matches[0]  # Already sorted in GPT filter
        raw_answer = best_match[0].metadata.get("a", "[No A]").strip()

    # âœ… Bulletify if long
    final_answer = bulletify_if_long(raw_answer)

    # â¬‡ï¸ Add visible spacing if the answer starts with bullet points
    if final_answer.strip().startswith("â€¢"):
        final_answer = "\n\n" + final_answer

        print(f"âœ… Answer selected after GPT topic filtering: {raw_answer}")

    else:
        final_answer = "Sorry, no valid answers found after GPT topic filtering."

    # ğŸ’¬ Append to conversation history
    final_answer += "\n\nIf this didnâ€™t resolve the issue, let me know."
    th_state["conversation_history"].append({"role": "user", "content": user_question})
    th_state["conversation_history"].append({"role": "assistant", "content": final_answer})
    return final_answer





